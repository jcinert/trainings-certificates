{"cells":[{"cell_type":"markdown","metadata":{"id":"uZYTGqGJ6Yqa"},"source":["# Introduction to Transformers\n","1. **A description of a Transformer model**\n","2. **Pre-training a language model**\n","3. **Overview of specific tasks**\n","4. **How to manipulate the output, without re-training the model (generation strategies etc.)**\n","5. **Hands-on: Logit Processors**"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"Qh2e3Ua56Yqf","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685689591408,"user_tz":-120,"elapsed":21418,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}},"outputId":"8346c68b-984d-4f11-ccbf-e7a10b65c250"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers[sentencepiece]==4.19.1\n","  Downloading transformers-4.19.1-py3-none-any.whl (4.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m54.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (3.12.0)\n","Collecting huggingface-hub<1.0,>=0.1.0 (from transformers[sentencepiece]==4.19.1)\n","  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m26.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (1.22.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (23.1)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (6.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (2022.10.31)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (2.27.1)\n","Collecting tokenizers!=0.11.3,<0.13,>=0.11.1 (from transformers[sentencepiece]==4.19.1)\n","  Downloading tokenizers-0.12.1-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (4.65.0)\n","Collecting sentencepiece!=0.1.92,>=0.1.91 (from transformers[sentencepiece]==4.19.1)\n","  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m78.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.10/dist-packages (from transformers[sentencepiece]==4.19.1) (3.20.3)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]==4.19.1) (2023.4.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers[sentencepiece]==4.19.1) (4.5.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.19.1) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.19.1) (2022.12.7)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.19.1) (2.0.12)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[sentencepiece]==4.19.1) (3.4)\n","Installing collected packages: tokenizers, sentencepiece, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.15.1 sentencepiece-0.1.99 tokenizers-0.12.1 transformers-4.19.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.0.1+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.0)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.11.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.1)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.0.0)\n","Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (3.25.2)\n","Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch) (16.0.5)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"]}],"source":["import sys\n","!{sys.executable} -m pip install transformers[sentencepiece]==4.19.1\n","!{sys.executable} -m pip install torch"]},{"cell_type":"markdown","metadata":{"id":"WEflKeEF6Yqg"},"source":["## Why Transformers?\n","\n","The transformers structure was introduced regarding the task of **Machine Translation**. Models, that have been used for this task prior were RNN's (recurrent neural networks) models. Their main problems were: being slow and dropping performance with longer sentences (inputs).\n","\n","What do the Transformers do differently?\n","* The **input sentence is processed at once**, where the RNN processed the sentence word by word. This allows for parallelization and thus improving the speed \n","* Using the **\"self-attention\"** and the sentence being processed at once helps to keep the information about dependencies intact, where the RNN approach was that the dependencies with the other words were passed down in the hidden states and as such the information was lost the longer the output sentence was.\n","* Apart from having word embeddings, which is a vector representation of the input, the transformers also have positional embeddings, because the sentence is no longer processed word by word, so we need a different way to include the information about the position of the word in the sentence. \n"]},{"cell_type":"markdown","metadata":{"id":"VBdZbIg46Yqh"},"source":["## 1. 🏞️ The journey from the input..."]},{"cell_type":"markdown","metadata":{"id":"SuQm6hAl6Yqi"},"source":["First we need to tokenize the input. Tokenizers are basically a dictionary where each word/ syllable/character has an ID. You **always** need to **use the tokenizer that was used during the pre-training and fine-tuning of your model**, otherwise you will get complete gibberish. The tokenizer also has special tokens such as EOS (end of sentence), BOS (beggining of sentence) etc. (+ you can add special tokens)\n","\n","When training (or in inference) it is possible, that you will want to process multiple inputs at once. As the model works with tensors, we need the inputs tokenized to input_ids of the same length. Because of that, the tokenized input also has the \"attention mask\" which is an indicator which parts of the tokenized sequence are relevant and which are only the padding for ensuring the same length requirement."]},{"cell_type":"code","execution_count":16,"metadata":{"id":"LvOakvMY6Yqj","outputId":"1274fb50-7da9-4090-8783-30e139b84c5c","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685693267389,"user_tz":-120,"elapsed":1542,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Original output: ['This is the supercalifragilisticexpialidocious input text.', 'second text']\n","Tokenized output (IDs): [[100, 19, 8, 1355, 15534, 20791, 173, 3040, 994, 102, 23, 4288, 7171, 2936, 3785, 1499, 5, 1], [511, 1499, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","Tokenized output (attention mask): [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]\n","Tokenized output (in tokens) - 1st sequence: ['▁This', '▁is', '▁the', '▁super', 'cali', 'frag', 'il', 'istic', 'ex', 'p', 'i', 'ali', 'doc', 'ious', '▁input', '▁text', '.', '</s>']\n","Tokenized output (in tokens) - 2nd sequence: ['▁second', '▁text', '</s>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"]}],"source":["from transformers import AutoTokenizer\n","\n","\n","tokenizer_t5 = AutoTokenizer.from_pretrained(\"t5-small\") # Download the corresponding tokenizer\n","input_sentence = [\"This is the supercalifragilisticexpialidocious input text.\", \"second text\"]\n","\n","# We enable padding, as we need both sentence tokenized to the sequence of the same length\n","tokenized_input = tokenizer_t5(input_sentence, padding=True)\n","\n","print(\"Original output: {}\".format(input_sentence))\n","print(\"Tokenized output (IDs): {}\".format(tokenized_input.input_ids))\n","\n","# attention mask vector provides information on which parts of the sample are relevant (are not padding)\n","print(\"Tokenized output (attention mask): {}\".format(tokenized_input.attention_mask))\n","\n","# The tokenizer has a way of dealing with words which are not in its vocab.\n","# Here if the word start after a whitespace it has \"_\" in front whereas if the word has to be split its parts have tokens without the \"_\"\n","print(\"Tokenized output (in tokens) - 1st sequence: {}\".format(tokenizer_t5.convert_ids_to_tokens(tokenized_input.input_ids[0])))\n","print(\"Tokenized output (in tokens) - 2nd sequence: {}\".format(tokenizer_t5.convert_ids_to_tokens(tokenized_input.input_ids[1])))\n"]},{"cell_type":"markdown","metadata":{"id":"hdav9j6G6Yqk"},"source":["### 1.1 Embeddings\n","The tokenized input then needs to be transformed into a vector representation. This representantion needs to hold information about the semantic property of a word (*Input Embedding*) and positional information about where the word is in the input sequence (*Positional Embedding*). Each word's embedding has size equivalent of the hidden states (default 512). \n"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"qbz31pZ_6Yql","outputId":"f155e727-102a-470e-cbe4-fef65da2df45","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685693538881,"user_tz":-120,"elapsed":2505,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Size of the input ids tensor: torch.Size([1, 18])\n","Size of the word embedding for the input sequence: torch.Size([1, 18, 512])\n"]}],"source":["from transformers import T5ForConditionalGeneration\n","\n","input_sentence = [\"This is the supercalifragilisticexpialidocious input text.\"]\n","model_t5 = T5ForConditionalGeneration.from_pretrained(\"t5-small\")\n","input_ids = tokenizer_t5(input_sentence, return_tensors=\"pt\")\n","outputs=model_t5.generate(input_ids.input_ids, return_dict_in_generate=True,output_hidden_states=True, output_attentions=True, output_scores=True)\n","\n","#encoder hidden states contain word embedding and then all outputs of each encoder layer\n","word_embeddings_for_input_sentence = outputs.encoder_hidden_states[0] \n","print(\"Size of the input ids tensor: {}\".format(input_ids.input_ids.shape))\n","print(\"Size of the word embedding for the input sequence: {}\".format(word_embeddings_for_input_sentence.shape))"]},{"cell_type":"code","source":["word_embeddings_for_input_sentence"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D51ws4wqKkyK","executionInfo":{"status":"ok","timestamp":1685693551752,"user_tz":-120,"elapsed":228,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}},"outputId":"35ac549e-e1a5-429f-bbe3-13195ac38398"},"execution_count":18,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor([[[ 14.9375,  -0.6211,  32.0000,  ..., -28.3750,  16.7500,  32.2500],\n","         [  4.5625,  -1.1953, -13.9375,  ...,  24.5000,  -2.9375,  11.5000],\n","         [ 12.1875,   6.7812,  10.2500,  ...,  11.7500,   6.2812,  45.5000],\n","         ...,\n","         [-20.5000,   6.3125,  -9.0625,  ...,  -0.0713, -40.7500,   5.1875],\n","         [ -4.2812,   7.4062, -14.5625,  ...,  13.0625,  -2.7344,  -3.1406],\n","         [ 12.6250,   8.1875, -11.6250,  ...,   7.9375,  -7.3125,   0.9453]]])"]},"metadata":{},"execution_count":18}]},{"cell_type":"markdown","metadata":{"id":"5L18uRkz6Yql"},"source":["### 1.2 What is Self-attention?\n","Self-attention layer is a building block of the Transformer model, which is able to include the information about the connection between  individual words/tokens from the input sequence. Below you can see how the attention score is computed\n","\n","![attention_score.png](https://jalammar.github.io/images/t/self-attention-matrix-calculation-2.png)\n","[[source]](https://jalammar.github.io/illustrated-transformer/)\n","\n","Input and output of every self-attention layer are embeddings / hidden states.\n","\n","The embeddings are passed to three separate linear layers, which compute the desired matrices **query matrix Q**, **key matrix K** and a **value matrix V** by multiplying the embbedding by their own weights ($W^{Q}$, $W^{K}$, $W^{V}$).  \n","\n","Each word queries the scores to all words from the input sequence, this is done by matrix multiplication of $Q$ and $K^T$. The results are divided (for stability purposes) and a softmax function (values in a row ad up to 1) is applied. The result is then multiplied with the matrix $V$. This applies the attention scores to the corresponding values (imagine $q_1$ being applied to all of matrix $K$ to be multiplied by $v_1$)\n","\n","#### 1.2.1 Multi-head attention\n","Instead of computing only one attention score we use the multi-head attention. What changes?\n","* ($W^{Q}$, $W^{K}$, $W^{V}$) are split and resized into a tuple of smaller ($W^{Q}$, $W^{K}$, $W^{V}$) for each head\n","* in each head Q, K and V is computed and the attention score is computed using $d_k$ = embedding size/ # heads\n","* the results for each head are concatonated together and transformed, so they could be used as input for another encoder/decoder.\n","\n","Allowing multiple attention score to be computed we allow for a richer representation.\n","\n","#### 1.2.2 Types of self-attention layers\n","\n","* Encoder self-attention (Multi-head attention) - Q,K and V are computed from the input sequence embedding or a hidden state of a previous encoder\n","* Decoder self-attention (Masked multi-head attention)- Q,K and V are computed from the target/output sequence embedding or a hidden state of a previous decoder\n","* Encoder-Decoder cross-attention (Multi-head attention)- Q is computed from the target/output sequence embedding or a hidden state of a previous decoder, K and V are from the last hidden state of the Encoder\n","\n","![attentions.png](https://jalammar.github.io/images/t/transformer_resideual_layer_norm_3.png)\n","[[source]](https://jalammar.github.io/illustrated-transformer/)"]},{"cell_type":"markdown","metadata":{"id":"fWmDTTAP6Yqm"},"source":["## 2.💪 Pre-training & Fine-tuning\n","The magic of Deep language models lays in their good preliminary knowledge of language. They obtain this knowledge during so-called pre-training phase, where they are trained for a different token classification task an instance of Language Modeling.\n","\n","\n","### 2.1 Language Modeling\n","\n","In the instances of Language Modeling task, models are asked to solve a task of 'guessing' the **right word in the context**.\n","\n","This task comes in two main instances:\n","\n","* **Masked Language Modeling (MLM)**: Models guess the correct token **within context**. This objective best prepares the model for **classification tasks** (Named Entity Recognition, Sequence Classification)\n","\n","![image.png](https://www.rohanawhad.com/content/images/size/w1600/2022/04/image.png)\n","[[source]](https://www.rohanawhad.com/improvements-of-spanbert-over-bert/)\n","\n","* **Causal Langauge Modeling (CLM)**: Models guess the **following token** from previous context. This objective is better for preparing the model for **generation**, such as Dialogue, Summarization, Translation.\n","\n","![CLM_1](https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/images/CLM_1_new.png?raw=1)  \n","![CLM_2](https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/images/CLM_2_new.png?raw=1)\n","![CLM_3](https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/images/CLM_3_new.png?raw=1)\n","\n","Other variances: Synthetic token classification (ALBERT), Mask infilling (BART, T5), Sequence reconstruction (BART), Sequence Classification (BERT).\n","\n","### 2.2 Fine-tuning\n","This is the process of training a pre-trained model for a specific task with new data (usually supervised learning). A specific task corresponds to a \"head\", aka a linear layer, which is trained to weight the last hidden states accordingly. "]},{"cell_type":"markdown","metadata":{"id":"dRDqBzMN6Yqn"},"source":["## 3.💡Specific tasks\n","As stated above Transformers can be used in multiple ways, depending on what the desired output is. We can either use the whole architecture as shown above or use only the decoder or the encoder. In every task the input is sequence of tokens. The output and how we aquire it depends on which task we choose. However we can sort them into two categories: **Token classification** and **Text generation**\n","\n","### 3.1 Token classification\n","For these tasks we need to have the best representation for each word within its context(left and right). Model pretrained on tasks such as MLM are best suited for token classification. For the classification itself, we want the best representation not a generated token, so we usually use the last hidden state of the encoder. \n","\n","* **Sequence classification (positive/negative)**: Before the sequence we wish to classify we add a $[CLS]$ token. We classify the sentence based on the value for the $[CLS]$ token.\n","\n","* **Token classification (NER)**: For each token we compute cross-entropy loss across all labels classify each token as the most probable label.\n","\n","* **Extractive QA**: For each token we compute cross-entropy loss to find the best candidates for start and end of the answer tokens in the sequence.\n","\n","### 3.2 Text generation\n","With these tasks the end goal is to guess the most probable token following the previous tokens. Model pre-trained on tasks similar to CLM are best suitable for text generation task. The following tasks have a certain overlap, as they both have text as input and output. These tasks need next token generation, which is provided by the decoder and as such can be performed by either a traditional Encoder-Decoder or Decoder-only Transformer.\n","\n","In both cases the embeddings are processed (either through a ED or D transformer) the \"best\" - in some cases most probable token (depends on the generation strategy) is generated.\n","\n","\n","* **Text generation (Completion, code generation)**: Given a sequence of words, the model generates the next word. This task can be trained on unlabeled data and is often used with Decoder-only transformers. \n","* **Sequence to sequence (Summarization, Translation, etc.)**: These tasks need the model to learn to map pairs of text (english to german, article to abstract). These tasks are mainly about transcribing the input into a different text (translate to a different language / simplify the text but maintain the meaning) and benefit from understanding the input sequence. Because of that they are used mainly with Encoder-Decoder models.\n","\n","![animation.gif](https://jalammar.github.io/images/t/transformer_decoding_2.gif)\n","[[source]](https://jalammar.github.io/illustrated-transformer/)\n","\n","### 3.3 Decoder or Encoder-Decoder?\n","With the rise of ChatGPT and seing its capabilities it is a valid question if a GPT (Decoder only) model would not suffice even on tasks, previously though better suited for a seq2seq task. A [Paper comparing Encoder-Decoder and Decoder-only models](https://arxiv.org/pdf/2304.04052.pdf) on machine translation showed some reasons, why we shouldn't focus only on decoder-only large language models for all text generation tasks. \n","\n","Reasons for using Decoder-only models:\n","* Smaller size and can be trained on much more data (unsupervised)\n","\n","Reasons against using Decoder-only models:\n","* No encoder hidden states in the decoder self-attention layers\n","* The only information about the input sequence is in the decoder hidden states\n","This results in more hallucination with the growing index of generation and the attention degeneration, as the hidden states cannot hold information about the input sequence\n","\n","The best rule-of-thunb would be: If you need the model to not diverge from initial input -> Encoder-Decoder. Would you greatly benefit from exposing the model to a large quantity of data and diverging from the input sequence meaning is not a massive drawback for you -> Decoder-only"]},{"cell_type":"markdown","metadata":{"id":"eKDUDiyF6Yqn"},"source":["## 4. 🏞️ The journey from the input...to the output - Text Generation\n","\n","Even though it may seem, that once we have a trained model we are done, there is the generation itself. While the weights in the decoder (and encoder) are set, we can still tweak the generated output, by using multiple generation strategies as well as using Logit Processors."]},{"cell_type":"markdown","metadata":{"id":"3Xw-j-SG6Yqn"},"source":["### 4.1 🗺️ Decoding strategies\n","While iteratively generating the output tokens, there are multiple strategies, that can be used to achieve the \"best\" output text."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"IvmUwMpt6Yqn","colab":{"base_uri":"https://localhost:8080/","height":232,"referenced_widgets":["56d263c441cd4772af3af074e53b091e","c699817de09e4fb39501f3e5cae6a0ca","b6ffde8c408b4b439d26813824c0b3ef","1c84106c654844e4ba7af353f1a72f74","ba543527305d449c8e818e4cbd9c0d24","10ed47da646049cc8ec0f815539b398f","f25c9d1af59d40fc905dc06974f0db01","5a836bbc2a3443d589ae46ec76437a93","c654afa7318448fc981223bf83b0a0bf","d73780e6a88645859d49609b42355076","2716afc236d642278fb65093cd71cbdf","7e210b3c05de42828ed8755c14a7aa02","1f00f43e55bd46f5a0c01ee1c099d0a5","e7261ad0b1a8461ead557280052e8266","4eb8d7bbe1f74be7ade9bd754a427007","ff29839f3a584e0daaaaab2b629a6592","14cbb44ebdfd4a66b85dc3d5638b1a9d","0fb0f68c3f8d4b658618dbcccc51007d","c080bbbea88d4befa13f9b1e955a36c8","a63f32f5cc98436e8897e9d2763bd90a","04aa13d07035497e96f03ffe19a5c870","84229904b4b74842bf461dc8d6d91a70","9b29ce137f25424fa8773efbb37e829f","e574d270fabc4d28beb3b29d9eff5cb8","843e0298b691468686155c534f1fafe3","0ef9704be404420baee39b899eb19911","c006ae2325ca41a0989aa913bc59ccf8","958547aefcfc4f8b86a21229f39e6390","6e5e141681b34bbdadce97d44a974cb5","d2597e9c038445be9b394ce39f26b31e","c1bf4c69c95a4f258dfe0761f6e55d64","faa3846d519d4b8f8957e538286ab3b0","02493ceaebd54ab7ba7d37644b9221f4","58df22c3387645ef8e211f155dee5a02","c210340081964e7ea536337dd871f0d1","c0d3f43f7f074a6ab707cef5949829af","5c7e76d67a154e0e83ab3eac214cd02c","c2f6a475d63144f5be54237c1090ce96","970b5b92d3e746b2a07e29808bd91a9e","17acfcfda36045ef83f5b48def3cfe92","f066dbbdbae74508bf5bba9c397b1288","6ab44f9d09da45ba96396736232be138","55a8f1acfe3b4b8b9b7a11c5d0dd2ba6","1a0bd95ef38642bfb0db72baf3655ce8","3b255cf022ca43a09f9096f2427fdcf0","e7a65623b18f43a487643fac88228156","1615f5223f3946cbbd5706c32fe11884","f6982244a2514c499e2fe855ba839178","968b7322869047abaeb22c086fd8581b","75b978b311d14ef3a5ed92076112c2ff","2c011ebe4d18418884e5eb3be67f83b2","a55dc06178254febacd8a422d059511c","bd91f387f1474a7986a75e4e925274a8","1a4bf3dec64b48c89f67a9aab0d6575d","f454fc47d8ad430997c2c37377a7bc87"]},"executionInfo":{"status":"ok","timestamp":1685691792621,"user_tz":-120,"elapsed":17732,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}},"outputId":"c91319f3-ace1-473a-dde0-338a0fd704f1"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/transformers/models/auto/modeling_auto.py:921: FutureWarning: The class `AutoModelWithLMHead` is deprecated and will be removed in a future version. Please use `AutoModelForCausalLM` for causal language models, `AutoModelForMaskedLM` for masked language models and `AutoModelForSeq2SeqLM` for encoder-decoder models.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56d263c441cd4772af3af074e53b091e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/523M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e210b3c05de42828ed8755c14a7aa02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/0.99M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b29ce137f25424fa8773efbb37e829f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/446k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"58df22c3387645ef8e211f155dee5a02"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading:   0%|          | 0.00/1.29M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b255cf022ca43a09f9096f2427fdcf0"}},"metadata":{}}],"source":["from transformers import AutoModelWithLMHead, AutoTokenizer\n","input_sentence = \"Two thousand years ago, the \"\n","model_gpt = AutoModelWithLMHead.from_pretrained(\"gpt2\")\n","tokenizer_gpt = AutoTokenizer.from_pretrained(\"gpt2\")\n","input = tokenizer_gpt(input_sentence, return_tensors=\"pt\")\n"]},{"cell_type":"markdown","metadata":{"id":"2NHGNSTU6Yqo"},"source":["* **Greedy search**: At each iteration, the most probable token is generated (most probable token at a time)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"cFg9KidL6Yqo","outputId":"ad5c1a73-5c63-4483-dcce-d0db78e16701","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685691845192,"user_tz":-120,"elapsed":7519,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["['Two thousand years ago, the vernacular of the ancient Egyptians was the language of the gods. The language of the gods was the language of the gods. The language of the gods was the language of the gods. The language of the gods was']\n"]}],"source":["outputs=model_gpt.generate(**input, early_stopping=True, max_length=50) #greedy search\n","print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"xVt0Vtae6Yqo"},"source":["* **Beam search**: At each iteration $N$ number of beams with the best overall probability are stored. After the generation is complete the beam with highes overall probability is returned (most probable output as a whole)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"AjSZs-D86Yqo","outputId":"a68da135-efd8-49b5-fec1-7bf09681800a","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685691860209,"user_tz":-120,"elapsed":4638,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["['Two thousand years ago, the vernal equinox was the first time that the sun and moon met, and the sun and moon were the first time that the sun and moon met. The sun and moon were the first time that the sun', 'Two thousand years ago, the vernal equinox was the first time that the sun and moon met, and the sun and moon were the first time that the sun and moon met.\\n\\nThe sun and moon were the first time that']\n"]}],"source":["outputs=model_gpt.generate(**input, num_beams=2, num_return_sequences=2, max_length=50) #beam search\n","print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"TBbegXjt6Yqp"},"source":["* **Multinomial sampling**: sampling the token distribution \n","* **Top-$K$ sampling**: sampling only $K$ samples with the highest probabilities.  \n","* **Nucleus sampling (top-$p$ sampling)**: Computing a cumulative distribution function and sampling only till the cut-off at the $p$ quantile\n"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"UQqqGMAX6Yqp","outputId":"ef722e39-ad15-4991-d143-d4e9d346c629","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685691903066,"user_tz":-120,"elapsed":7711,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["['Two thousand years ago, the ābrijrī of Pāṇaṃṇa and his disciple Suryasāti, his son Parāna, lived at the base of a mountain in the eastern Himal']\n"]},{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["[\"Two thousand years ago, the vernacular, to use Sir John Hawkins's expression, in the midst of the Indian War and the first outbreak of what was called the Indian War, was very far gone from the English. It was at times rather\"]\n","[\"Two thousand years ago, the vernacular 'half-game' (see game statistics) was as much about finding the action and finishing a piece as it was about 'finding where you're going with all the assists, being extra calculated.'\"]\n"]}],"source":["outputs=model_gpt.generate(**input, do_sample=True, max_length=50, early_stopping=True) # multinomial sampling\n","print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n","outputs=model_gpt.generate(**input, do_sample=True, top_k=50, max_length=50, early_stopping=True) # Top-k sampling\n","print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n","outputs=model_gpt.generate(**input, do_sample=True, top_p=0.90, top_k=0, max_length=50, early_stopping=True) #Top-p sampling\n","print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"Xn6LcZpk6Yqp"},"source":["* **Temperature**: A parameter which adjusts the logits before applying the softmax function. Greater temperature can lead to less probable words generated (more unexpected generated sequence), small temperature will lead to more conservative outputs. \n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"nA2HI6bx6Yqp","outputId":"064934a1-39ca-4a9d-8709-671ad663ac3b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685691983947,"user_tz":-120,"elapsed":6393,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n","Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["['Two thousand years ago, the vernal equinox was one of the two star systems that formed the world, and the second star, the vernal equinox, was nearly two million years old. The vernal equin']\n","['Two thousand years ago, the vernacular of the ancient Egyptians was the language of the gods. The language of the gods was the language of the gods. The language of the gods was the language of the gods. The language of the gods was']\n"]}],"source":["outputs=model_gpt.generate(**input, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature=0.7) #multinomial sampling with high temp\n","print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))\n","outputs=model_gpt.generate(**input, early_stopping=True, max_length=50, do_sample=True, top_k=0, temperature = 0.001) #multinomial sampling with low temp\n","print(tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True))"]},{"cell_type":"markdown","metadata":{"id":"WrGnINJt6Yqq"},"source":["### 4.2 👮 Logit Processors\n","Logit processors can be applied to the computed logits, to alter them and therefore change the next most probable token. They are not dependant on any decoding strategy. Some of the basic usages are enforcing minimal or maximal length of the output, forbidding specific words etc. Here we will show you a naive approach to creating a Logit processor which enforces a token that should be in the output."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"lKunXn0s6Yqq","outputId":"2d9c8bf1-09d1-4505-f55b-0dcc835100ae","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685692866152,"user_tz":-120,"elapsed":8505,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"stream","name":"stdout","text":["{'input_ids': [13526], 'attention_mask': [1]}\n"]},{"output_type":"execute_result","data":{"text/plain":["['Two thousand years ago, the vernacular of English was a mixture between \"the old\" and dumb. The word for stupid is now used to describe people who are not smart enough or have no sense in how they think about things (e-mail).\\nThe term has been around since at least 1750 when it came into use as an insult against those with intellectual disabilities: It\\'s called \\'dumb\\' because you\\'re too clever; its meaning comes from that one sentence which says something like this : You can\\'t be intelligent if your brain doesn´t work properly. This means there isn` t any way anyone could learn anything by doing so! So what do we call them?']"]},"metadata":{},"execution_count":15}],"source":["\n","from transformers import LogitsProcessor, LogitsProcessorList, BatchEncoding\n","import torch\n","\n","class EnforceWordProcessor(LogitsProcessor):\n","    def __init__(self, desired_input: BatchEncoding, param: float = 5.5):\n","        print(desired_input)\n","        assert len(desired_input.input_ids) ==1\n","        self.desired_input = desired_input.input_ids[0]\n","        self.param = param #the parameter which indicates how much we want to enforce the word\n","        pass\n","\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","        last_id = input_ids[-1][-1] # ID of the last generated token (whose score we are adjusting)\n","        if last_id != self.desired_input:\n","            scores[-1,self.desired_input] = scores[-1,self.desired_input]*self.param\n","        else:\n","            scores[-1,self.desired_input] = -float(\"inf\") # we only want to generate the token once\n","        return scores\n","    \n","enforced_word = \" dumb\" # we need the whitespace for the tokenizer to understand this as one token only\n","\n","custom_processor = EnforceWordProcessor(tokenizer_gpt(enforced_word, add_special_tokens=False))\n","\n","inputs = tokenizer_gpt(input_sentence, return_tensors=\"pt\")\n","\n","#repetition penalty is needed as the naive Logit Processor leads to repetition, so to avoid that, we include a rep. penalty\n","# Here we are using a beam search decodeing strategy, which is much more likely to fall into the pit of repetition\n","outputs = model_gpt.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor]), max_length=140, top_k=50, repetition_penalty=3.0)\n","output_text = tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True)\n","\n","output_text"]},{"cell_type":"markdown","metadata":{"id":"G2A8OYzN6Yqq"},"source":["### Are Logit Processors worth the work?\n","\n","The are use-cases, where a logit processor can help you enforce a rule of how structured the output should be. A really good example of that is the [**jsonformers**](https://github.com/1rgs/jsonformer) repo."]},{"cell_type":"markdown","metadata":{"id":"GE5PmagB6Yqq"},"source":["## 5. ✋ Hands-on: Adjusting the output without retraining using Logit Processors\n","\n","In this first short session we would ask you, to take the previously shown naive Logit Processor and try to adjust it to perform better by: \n","* enforce not a single token word, but a sequence of multiple tokens.\n","* Try to fix the repetition problem without using a repetition penalty\n","\n","You can also experiment with different decoding strategies, which we talked about above."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"95UTU1Su6Yqr","outputId":"657711ec-96e5-472f-9d23-cf52d9ab5ea6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1685692051954,"user_tz":-120,"elapsed":1379,"user":{"displayName":"Jan Cinert","userId":"14303388322313837358"}}},"outputs":[{"output_type":"stream","name":"stderr","text":["Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"]},{"output_type":"execute_result","data":{"text/plain":["['This is the start of the sequence.\\n\\nThe first thing to do is to create a new']"]},"metadata":{},"execution_count":10}],"source":["from transformers import LogitsProcessor, LogitsProcessorList\n","import torch\n","\n","class YourVeryOwnLogitProcessor(LogitsProcessor):\n","    def __init__(self):\n","        pass\n","\n","    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor):\n","        # The __call__ method has to return scores\n","        return scores\n","   \n","custom_processor = YourVeryOwnLogitProcessor()\n","\n","text = \"This is the start of the sequence\"\n","\n","inputs = tokenizer_gpt(text, return_tensors=\"pt\") # tokenize the input sequence\n","outputs = model_gpt.generate(**inputs, logits_processor=LogitsProcessorList([custom_processor])) # generate the output - now using greedy search\n","output_text = tokenizer_gpt.batch_decode(outputs, skip_special_tokens=True) # decode the output\n","\n","output_text"]},{"cell_type":"markdown","source":["NOTE: more elegant solution is available in Hugging face - \"constraint generation\""],"metadata":{"id":"9rdIDHmZLqIU"}}],"metadata":{"kernelspec":{"display_name":".venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.6"},"orig_nbformat":4,"colab":{"provenance":[{"file_id":"https://github.com/gaussalgo/L2L_MLPrague23/blob/main/notebooks/transformers_intro.ipynb","timestamp":1685689320011}]},"widgets":{"application/vnd.jupyter.widget-state+json":{"56d263c441cd4772af3af074e53b091e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c699817de09e4fb39501f3e5cae6a0ca","IPY_MODEL_b6ffde8c408b4b439d26813824c0b3ef","IPY_MODEL_1c84106c654844e4ba7af353f1a72f74"],"layout":"IPY_MODEL_ba543527305d449c8e818e4cbd9c0d24"}},"c699817de09e4fb39501f3e5cae6a0ca":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_10ed47da646049cc8ec0f815539b398f","placeholder":"​","style":"IPY_MODEL_f25c9d1af59d40fc905dc06974f0db01","value":"Downloading: 100%"}},"b6ffde8c408b4b439d26813824c0b3ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a836bbc2a3443d589ae46ec76437a93","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c654afa7318448fc981223bf83b0a0bf","value":665}},"1c84106c654844e4ba7af353f1a72f74":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d73780e6a88645859d49609b42355076","placeholder":"​","style":"IPY_MODEL_2716afc236d642278fb65093cd71cbdf","value":" 665/665 [00:00&lt;00:00, 13.3kB/s]"}},"ba543527305d449c8e818e4cbd9c0d24":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"10ed47da646049cc8ec0f815539b398f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f25c9d1af59d40fc905dc06974f0db01":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a836bbc2a3443d589ae46ec76437a93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c654afa7318448fc981223bf83b0a0bf":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d73780e6a88645859d49609b42355076":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2716afc236d642278fb65093cd71cbdf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e210b3c05de42828ed8755c14a7aa02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f00f43e55bd46f5a0c01ee1c099d0a5","IPY_MODEL_e7261ad0b1a8461ead557280052e8266","IPY_MODEL_4eb8d7bbe1f74be7ade9bd754a427007"],"layout":"IPY_MODEL_ff29839f3a584e0daaaaab2b629a6592"}},"1f00f43e55bd46f5a0c01ee1c099d0a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_14cbb44ebdfd4a66b85dc3d5638b1a9d","placeholder":"​","style":"IPY_MODEL_0fb0f68c3f8d4b658618dbcccc51007d","value":"Downloading: 100%"}},"e7261ad0b1a8461ead557280052e8266":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c080bbbea88d4befa13f9b1e955a36c8","max":548118077,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a63f32f5cc98436e8897e9d2763bd90a","value":548118077}},"4eb8d7bbe1f74be7ade9bd754a427007":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_04aa13d07035497e96f03ffe19a5c870","placeholder":"​","style":"IPY_MODEL_84229904b4b74842bf461dc8d6d91a70","value":" 523M/523M [00:10&lt;00:00, 82.9MB/s]"}},"ff29839f3a584e0daaaaab2b629a6592":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"14cbb44ebdfd4a66b85dc3d5638b1a9d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0fb0f68c3f8d4b658618dbcccc51007d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c080bbbea88d4befa13f9b1e955a36c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a63f32f5cc98436e8897e9d2763bd90a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"04aa13d07035497e96f03ffe19a5c870":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"84229904b4b74842bf461dc8d6d91a70":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9b29ce137f25424fa8773efbb37e829f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e574d270fabc4d28beb3b29d9eff5cb8","IPY_MODEL_843e0298b691468686155c534f1fafe3","IPY_MODEL_0ef9704be404420baee39b899eb19911"],"layout":"IPY_MODEL_c006ae2325ca41a0989aa913bc59ccf8"}},"e574d270fabc4d28beb3b29d9eff5cb8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_958547aefcfc4f8b86a21229f39e6390","placeholder":"​","style":"IPY_MODEL_6e5e141681b34bbdadce97d44a974cb5","value":"Downloading: 100%"}},"843e0298b691468686155c534f1fafe3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2597e9c038445be9b394ce39f26b31e","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c1bf4c69c95a4f258dfe0761f6e55d64","value":1042301}},"0ef9704be404420baee39b899eb19911":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_faa3846d519d4b8f8957e538286ab3b0","placeholder":"​","style":"IPY_MODEL_02493ceaebd54ab7ba7d37644b9221f4","value":" 0.99M/0.99M [00:00&lt;00:00, 2.14MB/s]"}},"c006ae2325ca41a0989aa913bc59ccf8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"958547aefcfc4f8b86a21229f39e6390":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6e5e141681b34bbdadce97d44a974cb5":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2597e9c038445be9b394ce39f26b31e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c1bf4c69c95a4f258dfe0761f6e55d64":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"faa3846d519d4b8f8957e538286ab3b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02493ceaebd54ab7ba7d37644b9221f4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"58df22c3387645ef8e211f155dee5a02":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c210340081964e7ea536337dd871f0d1","IPY_MODEL_c0d3f43f7f074a6ab707cef5949829af","IPY_MODEL_5c7e76d67a154e0e83ab3eac214cd02c"],"layout":"IPY_MODEL_c2f6a475d63144f5be54237c1090ce96"}},"c210340081964e7ea536337dd871f0d1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_970b5b92d3e746b2a07e29808bd91a9e","placeholder":"​","style":"IPY_MODEL_17acfcfda36045ef83f5b48def3cfe92","value":"Downloading: 100%"}},"c0d3f43f7f074a6ab707cef5949829af":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f066dbbdbae74508bf5bba9c397b1288","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_6ab44f9d09da45ba96396736232be138","value":456318}},"5c7e76d67a154e0e83ab3eac214cd02c":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55a8f1acfe3b4b8b9b7a11c5d0dd2ba6","placeholder":"​","style":"IPY_MODEL_1a0bd95ef38642bfb0db72baf3655ce8","value":" 446k/446k [00:00&lt;00:00, 809kB/s]"}},"c2f6a475d63144f5be54237c1090ce96":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"970b5b92d3e746b2a07e29808bd91a9e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"17acfcfda36045ef83f5b48def3cfe92":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f066dbbdbae74508bf5bba9c397b1288":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6ab44f9d09da45ba96396736232be138":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55a8f1acfe3b4b8b9b7a11c5d0dd2ba6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1a0bd95ef38642bfb0db72baf3655ce8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b255cf022ca43a09f9096f2427fdcf0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7a65623b18f43a487643fac88228156","IPY_MODEL_1615f5223f3946cbbd5706c32fe11884","IPY_MODEL_f6982244a2514c499e2fe855ba839178"],"layout":"IPY_MODEL_968b7322869047abaeb22c086fd8581b"}},"e7a65623b18f43a487643fac88228156":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_75b978b311d14ef3a5ed92076112c2ff","placeholder":"​","style":"IPY_MODEL_2c011ebe4d18418884e5eb3be67f83b2","value":"Downloading: 100%"}},"1615f5223f3946cbbd5706c32fe11884":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_a55dc06178254febacd8a422d059511c","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_bd91f387f1474a7986a75e4e925274a8","value":1355256}},"f6982244a2514c499e2fe855ba839178":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_1a4bf3dec64b48c89f67a9aab0d6575d","placeholder":"​","style":"IPY_MODEL_f454fc47d8ad430997c2c37377a7bc87","value":" 1.29M/1.29M [00:00&lt;00:00, 2.46MB/s]"}},"968b7322869047abaeb22c086fd8581b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"75b978b311d14ef3a5ed92076112c2ff":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2c011ebe4d18418884e5eb3be67f83b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a55dc06178254febacd8a422d059511c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bd91f387f1474a7986a75e4e925274a8":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"1a4bf3dec64b48c89f67a9aab0d6575d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f454fc47d8ad430997c2c37377a7bc87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}